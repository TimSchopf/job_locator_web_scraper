{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usefull websites:\n",
    "\n",
    "Create a rotating proxy crawler in Python 3 https://codelike.pro/create-a-crawler-with-rotating-ip-proxy-in-python/  \n",
    "Advanced Scraping - Form Submission http://jonathansoma.com/lede/foundations-2017/classes/adv-scraping/advanced-scraping-form-submission/  \n",
    "Web Scraping 201: finding the API http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bosch_params = {'searchTerm':'thesis'}\n",
    "daimler_params = {_: 1555346176442}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a request object from given url with random user agent and random proxy\n",
    "def request(url:str,request_type='get',post_params={}) -> requests.models.Response:\n",
    "    \n",
    "    # check parameter requirements\n",
    "    if request_type != 'get' and request_type != 'post':\n",
    "        raise ValueError(\"request_type must be 'get' or 'post' of type str\")\n",
    "        \n",
    "    if type(post_params) is not dict:\n",
    "        raise ValueError(\"post_params must be of type dict()\")\n",
    "    \n",
    "    #  disable ANY annoying exception with a fallback to default user agent if anything goes wrong\n",
    "    ua = UserAgent(fallback = 'Mozilla/5.0 (Windows; U; Windows NT 6.1; zh-HK) AppleWebKit/533.18.1 (KHTML, like Gecko) Version/5.0.2 Safari/533.18.5')\n",
    "\n",
    "    # update saved database of user agents\n",
    "    # fake_useragent stores collected data at os temp dir\n",
    "    print('update user agent database')\n",
    "    try: \n",
    "        ua.update()\n",
    "    except:\n",
    "        print('user agent database update failed')\n",
    "    \n",
    "    print('get proxy list')\n",
    "    # will contain proxies [ip, port]\n",
    "    proxies = []\n",
    "\n",
    "    # retrieve latest proxies from website\n",
    "    proxies_req = requests.get(url='https://www.sslproxies.org/', headers={'user-agent': ua.random})\n",
    "    proxies_req.encoding = 'utf-8'\n",
    "\n",
    "    # create beatifulsoup instance, parse html and get section with proxy list of website\n",
    "    parsed_html = BeautifulSoup(proxies_req.text, 'html.parser')\n",
    "    proxies_table = parsed_html.find(id='proxylisttable')\n",
    "\n",
    "    # save proxies from proxy list of website in the array\n",
    "    for row in proxies_table.tbody.find_all('tr'):\n",
    "        proxies.append({\n",
    "        'ip':   row.find_all('td')[0].string,\n",
    "        'port': row.find_all('td')[1].string\n",
    "      })\n",
    "        \n",
    "    # retrieve a random index proxy\n",
    "    def random_proxy_idx():\n",
    "        return random.randint(0, len(proxies) - 1)\n",
    "    \n",
    "    print('try proxies')\n",
    "    # try if proxy server is running, if not try other random proxy and user agent, try maximum x times then exit loop\n",
    "    proxy_is_good = False\n",
    "    i = 0\n",
    "    x = 10\n",
    "    while not proxy_is_good:\n",
    "    \n",
    "        # get dict for random proxy \n",
    "        random_proxy = proxies[random_proxy_idx()]\n",
    "        random_proxy_dict = {'http': 'http://' + random_proxy['ip'] + ':' + random_proxy['port'],\n",
    "                        'https': 'https://' + random_proxy['ip'] + ':' + random_proxy['port']}\n",
    "        # select random user agent\n",
    "        user_agent = ua.random\n",
    "    \n",
    "        # try to get request object from url with random user agent and random proxy\n",
    "        try:\n",
    "            if request_type == 'get':\n",
    "                req = requests.get(url=url, headers={'user-agent': user_agent},proxies=random_proxy_dict,timeout=1)\n",
    "            elif request_type == 'post':\n",
    "                req = requests.post(url=url, headers={'user-agent': user_agent},proxies=random_proxy_dict,data=post_params,timeout=1)\n",
    "            else:\n",
    "                raise ValueError(\"request_type must be 'get' or 'post' of type str\")\n",
    "                \n",
    "        # if anything goes wrong try again with differnt user agent and proxy\n",
    "        except:\n",
    "            print('ERROR','Proxy:',random_proxy,'User Agent:',user_agent)\n",
    "            if i == x:\n",
    "                print('All',i,'proxies are not reachable')\n",
    "                break\n",
    "            i += 1\n",
    "        # if everything goes right, end loop and return request object\n",
    "        else:\n",
    "            proxy_is_good = True\n",
    "            print('SUCCESS','Proxy:',random_proxy,'User Agent:',user_agent)\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update user agent database\n",
      "get proxy list\n",
      "try proxies\n",
      "ERROR Proxy: {'ip': '101.51.141.46', 'port': '37858'} User Agent: Mozilla/5.0 (iPad; CPU OS 5_1 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko ) Version/5.1 Mobile/9B176 Safari/7534.48.3\n",
      "SUCCESS Proxy: {'ip': '92.114.234.206', 'port': '38713'} User Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_6; zh-cn) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27\n"
     ]
    }
   ],
   "source": [
    "# Bosch requires a get request and blocks post requests\n",
    "bosch_req = request('https://www.bosch.de/karriere/jobs/', request_type='get', post_params=bosch_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update user agent database\n",
      "get proxy list\n",
      "try proxies\n",
      "ERROR Proxy: {'ip': '121.101.190.246', 'port': '61641'} User Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\n",
      "ERROR Proxy: {'ip': '186.43.33.81', 'port': '35736'} User Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; rv:45.66.18) Gecko/20177177 Firefox/45.66.18\n",
      "ERROR Proxy: {'ip': '91.218.163.74', 'port': '50661'} User Agent: Mozilla/5.0 (Windows; U; Windows NT 6.1; cs-CZ) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27\n",
      "SUCCESS Proxy: {'ip': '180.183.122.147', 'port': '8213'} User Agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "# Daimler uses an API to send the data to the client.\n",
    "# The Daimler Carrer Website itself is just a HTML template without data.\n",
    "# Data and template render client side and are sent seperately to the client.\n",
    "# In this case we must find the API URL that returns the raw data (the Daimler API sends JSON).\n",
    "daimler_api_url = 'https://global-jobboard-api.daimler.com/v3/search/%7B%22LanguageCode%22%3A%22EN%22%2C%22ScoreThreshold%22%3A80%2C%22SearchParameters%22%3A%7B%22MatchedObjectDescriptor%22%3A%5B%22PositionID%22%2C%22PositionTitle%22%2C%22PositionURI%22%2C%22LogoURI%22%2C%22OrganizationName%22%2C%22Organization%22%2C%22Organization.MemberCode%22%2C%22ParentOrganization%22%2C%22PositionLocation.CityName%22%2C%22PositionLocation.Longitude%22%2C%22PositionLocation.Latitude%22%2C%22PositionIndustry.Name%22%2C%22JobCategory.Name%22%2C%22JobCategory.Code%22%2C%22CareerLevel.Name%22%2C%22CareerLevel.Code%22%2C%22Facet%3AParentOrganization%22%2C%22Facet%3APositionLocation.CityName%22%2C%22Facet%3APositionLocation.CountryName%22%2C%22PublicationStartDate%22%2C%22ParentOrganizationGenesisID%22%5D%2C%22FirstItem%22%3A0%2C%22CountItem%22%3A20%7D%2C%22SearchCriteria%22%3A%5B%7B%22CriterionName%22%3A%22PositionFormattedDescription.Content%22%2C%22CriterionValue%22%3A%5B%22data%22%5D%7D%2C%7B%22CriterionName%22%3A%22PublicationLanguage.Code%22%2C%22CriterionValue%22%3A%5B%22EN%22%5D%7D%2C%7B%22CriterionName%22%3A%22PublicationChannel.Code%22%2C%22CriterionValue%22%3A%5B%2212%22%5D%7D%2C%7B%22CriterionName%22%3A%22CareerLevel.Code%22%2C%22CriterionValue%22%3A%5B%2240%22%5D%7D%5D%7D?_=1555346176443'\n",
    "daimler_req = request(daimler_api_url, request_type='post', post_params=daimler_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "daimler_data = daimler_req.json()\n",
    "bosch_data = BeautifulSoup(bosch_req.text, 'html.parser')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
